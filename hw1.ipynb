{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "735415db",
   "metadata": {},
   "source": [
    "Я взяла четыре статьи про фигурное катание со sport.ru. Там выделены темы, например вид спорта, упоминающиеся лица и организации. Это скорее разметка именованых сущностей, чем ключевых слов, но именованые сущности тоже в какой-то мере ключевые слова. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b65184",
   "metadata": {},
   "source": [
    "Статьи:\n",
    "- [про допинг Камилы Валиевой](https://www.sports.ru/tribuna/blogs/dalniyles/3093830.html#supertop)\n",
    "- [почему Гран-при России не собирает стадионы](https://www.sports.ru/tribuna/blogs/beznedokrutov/3093597.html)\n",
    "- [про этап Гран-при в Шеффилде](https://www.sports.ru/tribuna/blogs/vsemlutz/3092815.html)\n",
    "- [про дебют Софьи Муравьевой](https://www.sports.ru/tribuna/blogs/kissnotcry/3092657.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "377f97bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "164873f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# тут лежат тексты и ключевые слова\n",
    "df = pd.read_csv('key_corpus.csv', sep='%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91e48bf",
   "metadata": {},
   "source": [
    "Я выделяла не только именованные сущности, но и важные коллокации (например, *тройной аксель* в статье про Софью Муравьеву) и слова (например, *скольжение* или *компоненты* там же)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe4c40f",
   "metadata": {},
   "source": [
    "## Автоматическое извлечение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8426b3",
   "metadata": {},
   "source": [
    "### RAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3e00914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import RAKE\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from pymorphy2.tokenizers import simple_word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "004c1ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('russian')\n",
    "rake = RAKE.Rake(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c387db06",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MorphAnalyzer()\n",
    "def normalize_text(text):\n",
    "    lemmas = []\n",
    "    for t in simple_word_tokenize(text):\n",
    "        lemmas.append(\n",
    "            m.parse(t)[0].normal_form\n",
    "        )\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625950c8",
   "metadata": {},
   "source": [
    "Здесь можно было бы сделать собственный minFrequency для каждого текста, но это утомительно. Я выбрала 2, потому что так на всех текстах получалось что-то осмысленное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67787f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "rake_list = [rake.run(normalize_text(text), \n",
    "                      maxWords=3, \n",
    "                      minFrequency=2) for text in df.texts]\n",
    "rake_list = [ ', '.join([el[0] for el in rl]) for rl in rake_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67e1f04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rake'] = rake_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b0a253",
   "metadata": {},
   "source": [
    "### TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb63af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ceccfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "TR_list = [keywords.keywords(normalize_text(text), \n",
    "                             language='russian', \n",
    "                             additional_stopwords=stop, \n",
    "                             scores=True) for text in df.texts]\n",
    "TR_list = [ ', '.join([el[0] for el in tr]) for tr in TR_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15e5623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['textrank'] = TR_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85edf266",
   "metadata": {},
   "source": [
    "### keyBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "063d06c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 13:13:04.517892: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-18 13:13:04.600686: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-18 13:13:04.618992: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-18 13:13:04.915865: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-18 13:13:04.915903: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-18 13:13:04.915907: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5efb36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonauna/.local/lib/python3.10/site-packages/transformers/configuration_utils.py:369: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "kw_model = KeyBERT('clips/mfaq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "223a5c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "KB_list = [kw_model.extract_keywords(normalize_text(text), \n",
    "                                     keyphrase_ngram_range=(1, 1), \n",
    "                                     stop_words=stop, \n",
    "                                     top_n=20) for text in df.texts]\n",
    "KB_list = [ ', '.join([el[0] for el in kb]) for kb in KB_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d9892e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['keybert'] = KB_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c393da",
   "metadata": {},
   "source": [
    "## Шаблоны"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38bb2f9",
   "metadata": {},
   "source": [
    "Я решила сначала выделить в тексте всё, что соответствует шаблону, а потом сделать пересечение ключевых слов и всего, что соответствует шаблону. \n",
    "Конечно, можно было применить шаблон непосредственно к ключевым словам, но для этого надо было пропускать через пайморфи начальные формы, полученные через пайморфи, и мне показалось это аморальным. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71b3c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = ',.<>!@\"\\/\\'():;-–«»?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb359dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat1 = ['NOUN', \n",
    "        'ADJF', \n",
    "        'None']\n",
    "pat2 = [['ADJF', 'NOUN'], \n",
    "        ['NOUN', 'NOUN'], \n",
    "        ['NOUN', 'ADJF'], \n",
    "        ['None', 'None']]\n",
    "pat3 = [['ADJF', 'ADJF', 'NOUN'], ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60d4399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(text):\n",
    "    lemmas = []\n",
    "    for t in simple_word_tokenize(text):\n",
    "        if t not in punctuation:\n",
    "            p = m.parse(t)[0]\n",
    "            pos = p.tag.POS\n",
    "            if not pos:\n",
    "                pos = 'None'\n",
    "            lemmas.append([p.normal_form, str(pos), 0])\n",
    "            \n",
    "    good3 = []\n",
    "    for i in range(len(lemmas)-2):\n",
    "        if [lemmas[i][1], lemmas[i+1][1], lemmas[i+2][1]] in pat3:\n",
    "            good3.append([lemmas[i][0], lemmas[i+1][0], lemmas[i+2][0]])\n",
    "            lemmas[i][2], lemmas[i+1][2], lemmas[i+2][2] = 1, 1, 1\n",
    "\n",
    "    good2 = []\n",
    "    for i in range(len(lemmas)-1):\n",
    "        if [lemmas[i][1], lemmas[i+1][1]] in pat2 and (lemmas[i][2] == 0 or lemmas[i+1][2] == 0):\n",
    "            good2.append([lemmas[i][0], lemmas[i+1][0]])\n",
    "            lemmas[i][2], lemmas[i+1][2] = 1, 1\n",
    "\n",
    "    good1 = []\n",
    "    for i in range(len(lemmas)):\n",
    "        if lemmas[i][1] in pat1 and lemmas[i][2] == 0:\n",
    "            good1.append(lemmas[i][0])\n",
    "            lemmas[i][2] = 1\n",
    "            \n",
    "    good2 = [' '.join(el) for el in good2]\n",
    "    good3 = [' '.join(el) for el in good3]\n",
    "    good = good1 + good2 + good3\n",
    "\n",
    "    return ', '.join(good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d635be9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = [mask(text) for text in df.texts]\n",
    "df['masks'] = masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c933086",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_rake_list = [set([el[0] for el in rake.run(normalize_text(text), \n",
    "                                               maxWords=3, \n",
    "                                               minFrequency=2)]) & set(mask.split(', ')) \n",
    "                  for text, mask in zip(df.texts, df.masks)]\n",
    "\n",
    "mask_rake_list = [ ', '.join(el) for el in mask_rake_list]\n",
    "df['mask_rake'] = mask_rake_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec8bac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_TR_list = [set([el[0] for el in keywords.keywords(normalize_text(text), \n",
    "                                                         language='russian', \n",
    "                                                         additional_stopwords=stop, \n",
    "                                                         scores=True)]) & set(mask.split(', ')) \n",
    "                for text, mask in zip(df.texts, df.masks)]\n",
    "\n",
    "mask_TR_list = [ ', '.join(el) for el in mask_TR_list]\n",
    "df['mask_textrank'] = mask_TR_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8187967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_KB_list = [set([el[0] for el in kw_model.extract_keywords(normalize_text(text), \n",
    "                                                               keyphrase_ngram_range=(1, 1), \n",
    "                                                               stop_words=stop, \n",
    "                                                               top_n=20)]) & set(mask.split(', ')) \n",
    "                for text, mask in zip(df.texts, df.masks)]\n",
    "\n",
    "mask_KB_list = [ ', '.join(el) for el in mask_KB_list]\n",
    "df['mask_keybert'] = mask_KB_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5f96202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# здесь я пробовала искать ключевые слова исключительно в тех сочетаниях, которые соответствуют шаблону\n",
    "# (потому что я сначала неправильно поняла формулировку этого пункта)\n",
    "# могу сказать что это повышает recall до 0.3 в rake и textrank, а в keybert понижает до 0.07\n",
    "# а precision сильно падает до <0.1\n",
    "# но это всё вне задания\n",
    "\n",
    "# mask_rake_list = [rake.run(normalize_text(text), maxWords=3, minFrequency=2) for text in df.masks]\n",
    "# mask_rake_list = [ ', '.join([el[0] for el in rl]) for rl in mask_rake_list]\n",
    "# df['mask_rake'] = mask_rake_list\n",
    "\n",
    "# mask_TR_list = [keywords.keywords(normalize_text(text), language='russian', additional_stopwords=stop, scores=True) for text in df.masks]\n",
    "# mask_TR_list = [ ', '.join([el[0] for el in tr]) for tr in mask_TR_list]\n",
    "# df['mask_textrank'] = mask_TR_list\n",
    "\n",
    "# mask_KB_list = [kw_model.extract_keywords(normalize_text(text), keyphrase_ngram_range=(1, 1), stop_words=stop, top_n=20) for text in df.masks]\n",
    "# mask_KB_list = [ ', '.join([el[0] for el in kb]) for kb in mask_KB_list]\n",
    "# df['mask_keybert'] = mask_KB_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9888168b",
   "metadata": {},
   "source": [
    "Здесь я привожу ключевые слова из золотого стандарта к начальным формам, чтобы их можно было сравнивать с ключевыми словами, полученными автоматически."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a90b603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_standard = []\n",
    "for string in df.golden_standard:\n",
    "    new_string = []\n",
    "    for el in string.split(', '):\n",
    "        el = el.split(' ')\n",
    "        new_el = []\n",
    "        for word in el:\n",
    "            new_el.append(m.parse(word)[0].normal_form)\n",
    "        new_string.append(' '.join(new_el))\n",
    "    new_standard.append(new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "945e43d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_standard = [', '.join(el) for el in new_standard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "578d7136",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pm_standard'] = new_standard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9260e4",
   "metadata": {},
   "source": [
    "Сохраним финальный датафрейм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e6dadf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('key_corpus.csv', index=False, sep='%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158aaa83",
   "metadata": {},
   "source": [
    "## Оценка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c8f86a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('key_corpus.csv', sep='%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be7426d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76805cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(column, df):\n",
    "    expected = df.pm_standard # move\n",
    "    observed = df[column]\n",
    "    \n",
    "    \n",
    "    pre = []\n",
    "    re = []\n",
    "    fs = []\n",
    "    for i in range(len(expected)):\n",
    "        exp = expected[i].split(', ')\n",
    "        obs = observed[i].split(', ')\n",
    "        \n",
    "        TP = len(set(exp) & set(obs)) + 0.0001\n",
    "        FP = len(set(obs) - set(exp)) + 0.0001 # only in model\n",
    "        FN = len(set(exp) - set(obs)) + 0.0001 # only in standard\n",
    "        \n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        Fscore = 2 * precision * recall / (precision + recall)\n",
    "        \n",
    "        pre.append(precision)\n",
    "        re.append(recall)\n",
    "        fs.append(Fscore)\n",
    "    return mean(pre), mean(re), mean(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd14321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pd.DataFrame({'rake' : evaluate('rake', df), \n",
    "                      'textrank': evaluate('textrank', df),\n",
    "                      'keyBert':evaluate('keybert', df),\n",
    "                      'rake+mask':evaluate('mask_rake', df),\n",
    "                      'textrank+mask':evaluate('mask_textrank', df),\n",
    "                      'keyBert+mask':evaluate('mask_keybert', df)}, \n",
    "                     index = ['precision', 'recall', 'F-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0338f181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rake</th>\n",
       "      <th>textrank</th>\n",
       "      <th>keyBert</th>\n",
       "      <th>rake+mask</th>\n",
       "      <th>textrank+mask</th>\n",
       "      <th>keyBert+mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.090580</td>\n",
       "      <td>0.040301</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.132791</td>\n",
       "      <td>0.123337</td>\n",
       "      <td>0.262706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.172248</td>\n",
       "      <td>0.231183</td>\n",
       "      <td>0.139754</td>\n",
       "      <td>0.172248</td>\n",
       "      <td>0.231183</td>\n",
       "      <td>0.120523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F-score</th>\n",
       "      <td>0.117977</td>\n",
       "      <td>0.067489</td>\n",
       "      <td>0.115668</td>\n",
       "      <td>0.149814</td>\n",
       "      <td>0.156240</td>\n",
       "      <td>0.164797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               rake  textrank   keyBert  rake+mask  textrank+mask  \\\n",
       "precision  0.090580  0.040301  0.100004   0.132791       0.123337   \n",
       "recall     0.172248  0.231183  0.139754   0.172248       0.231183   \n",
       "F-score    0.117977  0.067489  0.115668   0.149814       0.156240   \n",
       "\n",
       "           keyBert+mask  \n",
       "precision      0.262706  \n",
       "recall         0.120523  \n",
       "F-score        0.164797  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e0e887",
   "metadata": {},
   "source": [
    "Там, где мы использовали маски, выросла точность, потому что маски уменьшают количество лишних выделений. При этом полнота не изменилась, потому что ничего нового мы не вынимаем. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbf9572",
   "metadata": {},
   "source": [
    "В стандарте люди выделены как имя+фамилия (*Евгений Плющенко*), а модели часто ловят только фамилию, и из-за этого получаются заниженные оценки."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ab079a",
   "metadata": {},
   "source": [
    "## Описание ошибок"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda1db8f",
   "metadata": {},
   "source": [
    "- Автоматические методы хорошо выделяют названия организаций, если это аббревиатуры (*русада*), но плохо выделяют полные названия (*спортивный арбитражный суд*). Ещё они плохо выделяют имена (*Камила Валиева*), и это может решаться отдельным выделением именованых сущностей, из которых потом выбираются ключевые слова.\n",
    "- Еще в выдаче есть стоп-слова, которых, видимо, не было в списке: *это*, *почему*. Можно расширить список стоп-слов.\n",
    "- Есть штуки типа *плющенко софья муравьёв пропустить*, внутри которых есть ключевые слова. Мне кажется, этого можно избежать, если добавить какую-то синтаксическую разметку. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
